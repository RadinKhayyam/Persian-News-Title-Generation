# Lightweight LLM Evaluation for Persian News Title Generation

## ðŸŽ¯ Project Overview

This repository documents a **focused technical and analytical evaluation of Lightweight Large Language Models (LLMs)** for the task of **Persian News Title Generation based on news content**.

The primary objective is to assess the **fine-tuning potential and rapid performance gains** of these models in generating titles that are **accurate, concise, and highly relevant**, aiming for maximum semantic and structural closeness to the "ground truth" titles found in the dataset.

This project specifically focuses on **Lightweight, Open-source LLMs** suitable for constrained computing environments, such as Google Colab, where efficient **fine-tuning** is feasible.

## ðŸ“ Dataset Details

The evaluation relies on a custom dataset sourced from real-world Persian news.

* **Size and Quality:** The dataset contains **over 1,000 pairs** of news `content` and corresponding `title` ground truths.
* **Ground Truth:** The titles were meticulously generated by **expert Persian news editors**, ensuring high editorial quality and journalistic standards for the target output.
* **Accessibility:** The raw dataset file will be included in this repository.

## ðŸ’¡ Model Selection Rationale

The evaluation was conducted using two lightweight, state-of-the-art open-source models known for their strong performance in understanding and generating the Persian language. The selection adheres to the task requirement of utilizing models under the $\approx 7-8$ billion parameter range:

| Model | Parameters | Source | Primary Focus | 
 | ----- | ----- | ----- | ----- | 
| **Gemma 3 - 4B - IT** | $4$ Billion | Google | Fine-Tuning Efficiency | 
| **Meta-Llama-3.1-8B-IT** | $8$ Billion | Meta | Advanced Base Performance | 

> While larger, proprietary models (e.g., Gemini or GPT families) might show superior zero-shot performance due to broader pre-training knowledge, this project emphasizes the performance gains achieved through **optimization and targeted fine-tuning** on resource-limited hardware.

## ðŸ›  Methodology: A Structured Four-Step Evaluation

The project was executed in a structured four-step process, applied to both the Gemma and Llama models.

### 1. Data Preparation and Exploratory Data Analysis (EDA)

The provided dataset (containing `content` and `title` columns) underwent extensive review:

* **Statistical Analysis:** Initial checks on text and title length distribution, vocabulary size, and frequency analysis.
* **Cleaning:** Identification and handling of potential noise, outliers, and corrupted data points.
* **Splitting:** Final data segregation into distinct **Training** and **Test** sets.

### 2. Phase I: Prompt-based Evaluation (Zero-Shot & Few-Shot)

The baseline performance of each model was established **without any fine-tuning**. This phase explored various prompting strategies:

* **Zero-shot:** Direct instruction to generate a title from the content.
* **Few-shot:** Inclusion of several $(Content, Title)$ pairs as examples within the prompt to guide the model's output style and format.
* **Prompt Engineering:** Iterative testing of different instructions, tones, and formatting to maximize baseline quality.

### 3. Phase II: Model Fine-Tuning via LORA

Following baseline assessment, both models were fine-tuned on the Training data.

* **Technique:** **LORA (Low-Rank Adaptation)** was utilized to achieve parameter-efficient fine-tuning (PEFT).
* **Goal:** Enable efficient training of relatively large models on GPUs with limited memory capacity.

### 4. Final Evaluation and Analysis

The performance across all phases (pre- and post-fine-tuning) was rigorously compared.

* **Quantitative Metrics:** Standard text generation metrics were employed: **BLEU, ROUGE-L, METEOR, and Bert Score**.
* **Qualitative Analysis:** A detailed human-in-the-loop review of output samples to assess **accuracy, fluency, and semantic coherence** with the source content.

## ðŸ“ˆ Key Results

The final evaluation confirmed the significant impact of LORA fine-tuning, dramatically improving metric scores compared to prompt-based methods.

### Gemma 3 - 4B Performance Across Scenarios

| Prompt Type | BLEU | ROUGE-L | METEOR | BERTScore |
| :--- | :--- | :--- | :--- | :--- |
| simple | 0.1386 | 0.2665 | 0.2276 | 0.7498 |
| zero\_shot | 0.1568 | 0.2987 | 0.3159 | 0.7669 |
| few\_shot | 0.2172 | 0.3776 | 0.3588 | 0.7939 |
| **refined (Base)** | **0.2401** | **0.4075** | **0.3901** | **0.8069** |
| **fine\_tuned (LORA)** | **0.3482** | **0.5215** | **0.5170** | **0.8481** |

### Llama 3.1 - 8B Performance Across Scenarios

| Prompt Type | BLEU | ROUGE-L | METEOR | BERTScore |
| :--- | :--- | :--- | :--- | :--- |
| simple | 0.1578 | 0.3001 | 0.2746 | 0.7649 |
| zero\_shot | 0.1599 | 0.3165 | 0.3508 | 0.7718 |
| few\_shot | 0.2123 | 0.3685 | 0.3821 | 0.7908 |
| **refined (Base)** | **0.2646** | **0.4248** | **0.4311** | **0.8106** |
| **fine\_tuned (LORA)** | **0.3798** | **0.5419** | **0.5283** | **0.8512** |
